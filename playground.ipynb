{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Using cached fastapi-0.110.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pydantic\n",
      "  Using cached pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.29.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi)\n",
      "  Using cached starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/igor/.pyenv/versions/3.11.6/envs/words/lib/python3.11/site-packages (from fastapi) (4.11.0)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.18.2 (from pydantic)\n",
      "  Using cached pydantic_core-2.18.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: click>=7.0 in /Users/igor/.pyenv/versions/3.11.6/envs/words/lib/python3.11/site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/igor/.pyenv/versions/3.11.6/envs/words/lib/python3.11/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /Users/igor/.pyenv/versions/3.11.6/envs/words/lib/python3.11/site-packages (from starlette<0.38.0,>=0.37.2->fastapi) (4.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/igor/.pyenv/versions/3.11.6/envs/words/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/igor/.pyenv/versions/3.11.6/envs/words/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi) (1.3.1)\n",
      "Using cached fastapi-0.110.3-py3-none-any.whl (91 kB)\n",
      "Using cached pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "Using cached pydantic_core-2.18.2-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m458.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "Installing collected packages: uvicorn, pydantic-core, annotated-types, starlette, pydantic, fastapi\n",
      "Successfully installed annotated-types-0.6.0 fastapi-0.110.3 pydantic-2.7.1 pydantic-core-2.18.2 starlette-0.37.2 uvicorn-0.29.0\n"
     ]
    }
   ],
   "source": [
    "# list all the packages mentioned in the code below\n",
    "# !pip install fastapi pydantic uvicorn\n",
    "# !pip install numpy pandas matplotlib seaborn scikit-learn sentence_transformers torch nltk annoy tqdm ipython jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\n",
    "    'sentence-transformers/all-mpnet-base-v2',\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/igor/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import re\n",
    "letters = re.compile(r'[a-z]', re.I)\n",
    "\n",
    "\n",
    "def clear(s):\n",
    "    s = s.lower()\n",
    "    # cut 's off the end of words\n",
    "    if s.endswith(\"'s\"):\n",
    "        s = s[:-2]\n",
    "\n",
    "    # cut off punctuation from both ends using a regex\n",
    "    s = re.sub(r'^[^a-z]+', '', s)\n",
    "    s = re.sub(r'[^a-z]+$', '', s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "freqs = nltk.FreqDist(\n",
    "    clear(w) for w in brown.words()\n",
    "    if letters.match(w)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorify(*strings: str):\n",
    "    strings = list(strings)\n",
    "    embeddings = model.encode(strings, convert_to_tensor=True)\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    embeddings = embeddings / torch.norm(embeddings, dim=1, keepdim=True)\n",
    "\n",
    "    return [emb.cpu().numpy() for emb in embeddings]\n",
    "\n",
    "vectorify(\"hello\")[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "words = list(freqs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "word2vec = {\n",
    "    word: emb \n",
    "    for word, emb in zip(words, vectorify(*words))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# save the word2vec model\n",
    "import pickle\n",
    "with open(\"word2vec2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word2vec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45457"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# load the word2vec model\n",
    "import pickle\n",
    "with open(\"word2vec.pkl\", \"rb\") as f:\n",
    "    word2vec = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import numpy as np\n",
    "\n",
    "f = word2vec['pig'].shape[0]  # Dimension of each vector (768)\n",
    "n_trees = 50  # More trees, more precision, more memory and build time\n",
    "\n",
    "# Initialize Annoy Index\n",
    "t = AnnoyIndex(f, 'angular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e114e240e634f36878fa9bd465a0d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add items to index\n",
    "\n",
    "for i, (word, emb) in enumerate(tqdm(word2vec.items())):\n",
    "    t.add_item(i, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.build(n_trees)\n",
    "\n",
    "t.save('word2vec2.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.load('word2vec.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "word2index = {word: i for i, word in enumerate(word2vec.keys())}\n",
    "index2word = {i: word for i, word in enumerate(word2vec.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "def get_distance(vec1, vec2):\n",
    "    return np.linalg.norm(vec1 - vec2)\n",
    "\n",
    "@lru_cache(maxsize=256)\n",
    "def get_coordinates(word):\n",
    "    return word2vec[word] if word in word2vec else vectorify(word)[0]\n",
    "\n",
    "def get_nearest_neighbors(word, n=10):\n",
    "    i = get_coordinates(word) if isinstance(word, str) else word\n",
    "    return [\n",
    "        (\n",
    "            index2word[j],\n",
    "            d,\n",
    "        )\n",
    "        for j, d in zip(*t.get_nns_by_vector(i, n, include_distances=True))\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12391e435fb3406c8c863da4ae398a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='love hate', description='words'), IntSlider(value=10, description='n', min=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "\n",
    "man = get_coordinates('man')\n",
    "woman = get_coordinates('woman')\n",
    "\n",
    "# get the line between those 2 points\n",
    "\n",
    "def point_on_line(a, b, t):\n",
    "    \"\"\"\n",
    "    Compute a point along the line between vectors a and b.\n",
    "    \n",
    "    Parameters:\n",
    "    - a (np.array): The starting point vector.\n",
    "    - b (np.array): The ending point vector.\n",
    "    - t (float): Interpolation parameter (0 <= t <= 1).\n",
    "                  t=0 returns a, t=1 returns b, and values in between return\n",
    "                  points along the line between a and b.\n",
    "    \n",
    "    Returns:\n",
    "    - np.array: A point along the line.\n",
    "    \"\"\"\n",
    "    return a + t * (b - a)\n",
    "\n",
    "def point_in_triangle(a, b, c, u, v, w):\n",
    "    \"\"\"\n",
    "    Compute a point inside the triangle formed by vectors a, b, and c using normalized barycentric coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    - a, b, c (np.array): The vertices of the triangle.\n",
    "    - u, v, w (float): Initial barycentric coordinates, which will be normalized so that their sum is 1.\n",
    "    \n",
    "    Returns:\n",
    "    - np.array: A point inside the triangle.\n",
    "    \"\"\"\n",
    "    # Stack the vertices and the coordinates into arrays\n",
    "    vertices = np.array([a, b, c])\n",
    "    weights = np.array([u, v, w])\n",
    "    \n",
    "    # Normalize the barycentric coordinates\n",
    "    weights_normalized = weights / weights.sum()\n",
    "    \n",
    "    # Compute the point in the triangle\n",
    "    return np.dot(weights_normalized, vertices)\n",
    "\n",
    "def f(words, n, masculinity=0.5, femininity=0.5, nonbinary=0.5):\n",
    "    wrds = words.split()\n",
    "    coordinates = [get_coordinates(word) for word in wrds]\n",
    "    median = np.sum(coordinates, axis=0)\n",
    "    gender = point_in_triangle(\n",
    "        get_coordinates('man'),\n",
    "        get_coordinates('woman'),\n",
    "        get_coordinates('non-binary'),\n",
    "        masculinity,\n",
    "        femininity,\n",
    "        nonbinary,\n",
    "    )\n",
    "    \n",
    "    return get_nearest_neighbors(median + gender, n)\n",
    "\n",
    "interact(\n",
    "    f,\n",
    "    words='love hate',\n",
    "    n=widgets.IntSlider(min=1, max=100, value=10), \n",
    "    masculinity=widgets.FloatSlider(min=0, max=1, value=0.5),\n",
    "    femininity=widgets.FloatSlider(min=0, max=1, value=0.5),\n",
    "    nonbinary=widgets.FloatSlider(min=0, max=1, value=0.5),   \n",
    ")\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadae94db6b045aa83a2609a51f9a606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='love joy', description='words'), IntSlider(value=10, description='n', min=2)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "def project_to_best_plane(points, n_components=2):\n",
    "    \"\"\"\n",
    "    Project points onto the best fitting plane using PCA.\n",
    "\n",
    "    Parameters:\n",
    "    - points (np.array): A NumPy array with shape (n_samples, n_features)\n",
    "                         where n_samples is the number of data points and\n",
    "                         n_features is the dimensionality of each data point.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: The projection of the points onto the best fitting plane.\n",
    "    \"\"\"\n",
    "    # Initialize PCA with 2 components\n",
    "    pca = PCA(n_components)\n",
    "    \n",
    "    # Fit PCA on the data and transform the data to the new axes\n",
    "    projected_points = pca.fit_transform(points)\n",
    "    \n",
    "    return projected_points\n",
    "\n",
    "\n",
    "def f(words, n):\n",
    "    wrds = set(words.split())\n",
    "    coordinates = [get_coordinates(w) for w in wrds]\n",
    "    median = np.sum(coordinates, axis=0)\n",
    "    neigbours: tuple[str, float] = get_nearest_neighbors(median, n)\n",
    "    found = set(w for w, _ in neigbours)\n",
    "\n",
    "    all_words: list[dict] = [\n",
    "        {\n",
    "            'word': w,\n",
    "            'distance': d,\n",
    "            'coordinates': get_coordinates(w),\n",
    "        }\n",
    "        for w, d in neigbours\n",
    "    ] + [\n",
    "        {\n",
    "            'word': w,\n",
    "            'distance': np.linalg.norm(c - median),\n",
    "            'coordinates': c,\n",
    "        }\n",
    "        for w, c in zip(wrds, coordinates)\n",
    "        if w not in found\n",
    "    ]\n",
    "\n",
    "    projected_points = project_to_best_plane(\n",
    "        [word['coordinates'] for word in all_words], n_components=3\n",
    "    )\n",
    "\n",
    "    for i, word in enumerate(all_words):\n",
    "        word['projected'] = projected_points[i]\n",
    "    \n",
    "\n",
    "    # plot the points, color is the distance from the median, alpha is the frequency from freqs dict\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    for word in all_words:\n",
    "        word['freq'] = freqs.get(word['word'], 1)\n",
    "\n",
    "    max_freq = max([word['freq'] for word in all_words])\n",
    "    min_freq = min([word['freq'] for word in all_words])\n",
    "\n",
    "    def normalize_freq(freq):\n",
    "        return (freq - min_freq) / (max_freq - min_freq)\n",
    "    \n",
    "    for word in all_words:\n",
    "        x, y, z = word['projected']\n",
    "        color = normalize_freq(word['freq'])\n",
    "        fr = np.log(word['freq']) * 10 + 1\n",
    "        alpha = max(0.4, 1 - word['distance'])\n",
    "        pallete = matplotlib.colormaps['viridis']\n",
    "        ax.scatter(x, y, z, s=fr, alpha=alpha, label=word)\n",
    "        ax.text(x, y, z, word['word'], fontsize=12, alpha=alpha)\n",
    "    plt.show()\n",
    "\n",
    "interact(\n",
    "    f,\n",
    "    words='love joy',\n",
    "    n=widgets.IntSlider(min=2, max=100, value=10), \n",
    ")\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/igor/.pyenv/versions/3.11.6/envs/tmp/bin/python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!which python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Darwin'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
